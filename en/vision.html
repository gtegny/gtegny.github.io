<!DOCTYPE HTML>
<!--
	Spectral by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Geoffrey Tegny - Computer vision projects</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="/assets/css/main.css" />
		<noscript><link rel="stylesheet" href="/assets/css/noscript.css" /></noscript>
					<style>
						.projects {
							display: flex;
							flex-direction: column;
							border: solid 1px lightgray;
							padding: 0 9rem;
							border-radius: 10px;
							margin-bottom: 5rem;
							min-width: 355px;
						}
					
						.projects>header {
							margin-top: 2rem;
						}
					
						@media screen and (max-width: 980px) {
							.projects {
								padding: 0 3rem;
							}
						}
					</style>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="/en/index.html">Geoffrey Tegny</a></h1>
						<div class="lang">
							<a class="language" href="/kr/building.html">Korean</a>
						</div>
						<nav id="nav">
							<ul>
								<li class="special">
									<a href="#menu" class="menuToggle"><span>Menu</span></a>
									<div id="menu">
										<ul>
											<li><a href="/en/index.html">Home</a></li>
											<li>
												<a href="#">Projects</a>
												<ul>
													<li><a href="/en/robotics.html">Robotics projects</a></li>
													<li><a href="/en/vision.html">Visions projects</a></li>
													<li><a href="/en/building.html">Smart building / industry projects</a></li>
													<li><a href="/en/others.html">Others projects</a></li>
												</ul>
							
											</li>
											<li><a href="/en/profile.html">My profile</a></li>
											<li><a href="/en/contact.html">Contact me</a></li>
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<article id="main">
						<header>
							<h2>My Computer Vision projects</h2>
							<p>Pose detection, Action detection, V-SLAM, Sensor fusion, Nvidia, CUDA, NeuralNetwork, Web server, C++, Python, Vue.js</p>
						</header>
						<section class="wrapper style5" style="padding-top: 2rem;">
							<div class="inner">

								<section class="projects">
									<header>
										<h2>AI vision server for robotic platform</h2>
										<p style="margin-bottom: 0.5rem;">Private company - Busan, South Korea</p> 
										<p style="margin-bottom: 0.5rem;"><i>2023~2024</i></p>
										<p style="margin-bottom: 0; font-size: 0.75rem;color: darkred;"><i>limited information due to a non-disclosure agreement</i></p>
										<hr />
									</header>
									<p style="overflow: auto;">
										<span class="image left" style="width: 300px;"><img src="/assets/images/VisionServer-POI.jpg" alt="VisionServer-POI"  /></span>
										<br/>
										The main goal of the project is to build a standalone web server that can run several vision-based AI applications.
										Combined with the developed robotic server, this controller allows Visual-SLAM.<br/>
										This server should have a small footprint to be embedded in a robotic platform.<br/>
										<br />
										I am the team leader and the solution designer of the project. 
										I worked on all the parts from server BE and FE and vision algorithm coding.<br />
										My team was composed of 2 developers and 3 interns.<br />
										<br /><br />
										<b>Keywords:</b>
										<br />
										C++, Python, Vuejs, Visual SLAM, R.Pi5, Jetson Nano, Jetson Nano Orin, CUDA, RGB, RGB-D, VIO, project manager, Notion
									</p>

									<button onclick="toggleExtraById()" style="margin-bottom: 20px; align-self: end;">Show more</button>

									<div class="extra" style="display: none;">
										<h4>Hardware points</h4>
										<p>
											Several architectures have been tested using controllers such as intel MiniPC (CPU), Nvidia Nano and Nano Orin but also 
											various cameras such as basic USB RGB cameras, RGB-D devices from Orbbec or RGB-D + IMU.<br/>
											The server also controls a servo motor to change the camera field of view.
										</p>
										<h4>Algorithm points</h4>
										<p>
											The main goal of the algorithms was to monitor multi-person poses and actions.
											For each solution, several algorithms have been developed to best fit the controller (CPU / GPU / Nvidia).<br />
											Severals algorithms were developed and we are in discussion with partners / customers about adapting them to answer specific needs:<br/>
											Multi-person trajectories tracking, dynamic multi-people pose detection, Person Of Interest tracking, multi-people action detection.<br/>
											Finally, we implemented V-SLAM using Visual Inertial Odometry algorithm and fused it to the previous SLAM solution we developed.
										</p>
										<h4>Web server points</h4>
										<p>
											The web server allows any device to connect to the server, run any algorithms or control the camera's position and change any hardware settings.<br/>
											The different hardware settings (hardware type, serial communication) can be changed directly in the UI of the server.<br />
											The server allows the use of an external video feed but also the use of an external computing server in case we need to
											run heavy algorithms that are not compatible with the embedded server.
										</p>
									</div>
									<div class="gallery">
										<img src="/assets/images/Vision-BestFeatures.jpg" alt="Image 1" onclick="openModal('/assets/images/Vision-BestFeatures.jpg')">
										<img src="/assets/images/Vision-PoseEstimation1.jpg" alt="Image 2" onclick="openModal('/assets/images/Vision-PoseEstimation1.jpg')">
									</div>
								</section>

								<section class="projects">
									<header>
										<h2>Visual Odometry and Visual SLAM</h2>
										<p style="margin-bottom: 0.5rem;">Private company - Busan, South Korea</p> 
										<p style="margin-bottom: 0.5rem;"><i>2023~2024</i></p>
										<p style="margin-bottom: 0; font-size: 0.75rem;color: darkred;"><i>limited information due to a non-disclosure agreement</i></p>
										<hr />
									</header>
									<p style="overflow: auto;">
										<span class="image left" style="width: 300px;"><img src="/assets/images/Vision-VSLAM_Nano.jpg" alt="VSLAM_Nano"  /></span>
										<br/>
										This project is part of the AI vision server but it focuses more on the localization and the navigation.
										In this project, we are implementing and testing several known solutions for the features detection, features mapping and loop closure.<br />
										The goal of this project is to understand the existing vision solutions for localization and to create a pipeline to be used for our visual SLAM.
										Our custom Visual SLAM is still in development, but we also implemented the existing Stella SLAM to be able to first test visual localization and navigation with our robot.
										<br />
										I helped 3 interns to learn about the visual algorithm, I am coding with them the different solutions and I am creating the global architecture with another robotic developer.<br />
										<br /><br />
										<b>Keywords:</b>
										<br />
										C++, Python, Vuejs, Visual SLAM, Jetson Nano Orin, CUDA, RGB, RGB-D, VIO, Features detection, Features mapping, Loop closure, Project manager
									</p>

									<button onclick="toggleExtraById()" style="margin-bottom: 20px; align-self: end;">Show more</button>

									<div class="extra" style="display: none;">
										<h4>Hardware points</h4>
										<p>
											We focus on running the algorithm on Nvida based PC then porting them on the Jetson Nano Orin.
											We are focusing on using mono-camera, IMU and odometry data from the hub motors, but we also implement a solution with a RGB-D camera.<br/>
										</p>
										<h4>Algorithm points</h4>
										<p>
											The different code parts have been implemented in Python then in C++. We are focusing on adapting some of the libraries to use CUDA available on the Nvidia devices.<br />
											For each solution part, different algorithms have been tested, such as FAST / HARRIS / SIFT / SURF for features dectection and ORB for features matching, 
											and we studied the pre-processing of the image using different filtering techniques (recoloring, edge filtering, ...).
										</p>
									</div>
								</section>
								<section class="projects">
									<header>
										<h2>Occupancy detection and people counting for Building management system</h2>
										<p style="margin-bottom: 0.5rem;">TegnyTech - Busan, South Korea</p>
										<p style="margin-bottom: 0.5rem;"><i>2023~2024</i></p>
										<hr />
									</header>
									<p style="overflow: auto;">
										<span class="image left" style="width: 300px;"><img src="/assets/images/Vision-TT-Yolo.jpg" alt="BEMS-occupancy" /></span>
										<br />
										The main goal of the project is to build a simple server that will receive video feeds from different rooms and return their 
										occupancy states.<br />
										The occupancy state of a room is important for building control; It allows energy savings and to better control the room comfort.
										Depending on the occupancy state, the lighting condition or even ventilation setpoints can be adjusted.
										Compared to traditional solutions using passive infrared, this solution allows to detect a person without a need of any motion.<br />
										<br />
										I am developing this server alone and integrating it in my BEMS solution to propose more offers to my partner company.<br />
										<br /><br />
										<b>Keywords:</b>
										<br />
										C++, Vuejs, R.Pi5, intel miniPC, Jetson Nano Orin, RGB, Person detection, ModbusTCP, BEMS, Energy savings
									</p>
								
									<button onclick="toggleExtraById()" style="margin-bottom: 20px; align-self: end;">Show more</button>
								
									<div class="extra" style="display: none;">
										<h4>Hardware points</h4>
										<p>
											The server can be directly installed on the BEMS computer or run into a standalone controller that is compatible with the number of video feeds.<br />
											Video feeds can be gathered via USB connection or directly from IP cameras.
										</p>
										<h4>Algorithm points</h4>
										<p>
											As speed is not important in building management, the algorithms are designed to run with low performance to the reduce cost of the product.<br />
											Several algorithms were developed to best fit the type of the controller where the server runs: YOLOV8, DetectNet, NeuralNetwork based.<br />
											The algorithms were developed using office videos databases such as "Edinburgh office monitoring video dataset".<br />
											Finally, the results of the concurrent occupancy detection on different video feeds are stored and can be polled by a
											Modbus TCP client.
										</p>
										

									</div>
								</section>
								

								
							</div>
						</section>

					</article>

				<!-- Footer -->
					<footer id="footer">
						<ul class="icons">
							<li><a href="assets/docs/Geoffrey_TEGNY-Resume_2024-Korean.pdf" target="blank"
									class="icon solid fa-file-pdf fa-2x"><span class="label">Resume</span></a></li>
							<li><a href="mailto:geoffrey.tegny@gmail.com" class="icon solid fa-envelope fa-2x"><span
										class="label">Email</span></a></li>
							<li><a href="https://www.linkedin.com/in/geoffreytegny" target="blank"
									class="icon solid brands fa-linkedin-in fa-2x"><span class="label">LinkedIn</span></a></li>
						</ul>
						<ul class="copyright">
							<li>&copy; GeoffreyTegny</li>
							<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>

			</div>

		<!-- Scripts -->
			<script>
				const toggleExtraById = () =>{
					let extra = event.target.parentNode.getElementsByClassName('extra')[0]
					if (extra.style.display == "none") {
						event.target.textContent = "Hide details"
						extra.style.display = "block"

					} else {
						event.target.textContent = "Show more"
						extra.style.display = "none"
					}
				}
				const openModal = (imageSrc) => {
						var modal = document.getElementById("imgViewer");
						var modalImg = document.getElementById("modalImg");
						modal.style.display = "block";
						modalImg.src = imageSrc;
					}
					const closeModal = () => {
						var modal = document.getElementById("imgViewer");
						modal.style.display = "none";
					}
					window.onclick = function (event) {
						var modal = document.getElementById("imgViewer");
						if (event.target == modal) {
							modal.style.display = "none";
						}
					}
			</script>
			<script src="/assets/js/jquery.min.js"></script>
			<script src="/assets/js/jquery.scrollex.min.js"></script>
			<script src="/assets/js/jquery.scrolly.min.js"></script>
			<script src="/assets/js/browser.min.js"></script>
			<script src="/assets/js/breakpoints.min.js"></script>
			<script src="/assets/js/util.js"></script>
			<script src="/assets/js/main.js"></script>



	</body>
</html>